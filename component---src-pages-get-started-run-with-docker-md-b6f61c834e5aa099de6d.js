(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{966:function(n,e,o){"use strict";o.r(e),o.d(e,"_frontmatter",(function(){return p})),o.d(e,"default",(function(){return s}));o(11),o(6),o(5),o(3),o(7),o(4),o(8),o(1);var r=o(78),t=o(959);function a(){return(a=Object.assign||function(n){for(var e=1;e<arguments.length;e++){var o=arguments[e];for(var r in o)Object.prototype.hasOwnProperty.call(o,r)&&(n[r]=o[r])}return n}).apply(this,arguments)}var p={};void 0!==p&&p&&p===Object(p)&&Object.isExtensible(p)&&Object.defineProperty(p,"__filemeta",{configurable:!0,value:{name:"_frontmatter",filename:"src/pages/Get_started/Run_with_Docker.md"}});var i={_frontmatter:p},c=t.a;function s(n){var e=n.components,o=function(n,e){if(null==n)return{};var o,r,t={},a=Object.keys(n);for(r=0;r<a.length;r++)o=a[r],e.indexOf(o)>=0||(t[o]=n[o]);return t}(n,["components"]);return Object(r.b)(c,a({},i,o,{components:e,mdxType:"MDXLayout"}),Object(r.b)("h1",{id:"running-spark-in-docker"},"Running Spark in Docker"),Object(r.b)("p",null,"Spark can run in a Docker. This is a sample Dockerfile for running Spark."),Object(r.b)("pre",null,Object(r.b)("code",a({parentName:"pre"},{}),'FROM mcr.microsoft.com/dotnet/core/aspnet:2.2-stretch-slim AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/core/sdk:2.2-stretch AS build\nWORKDIR /src\nCOPY ["./src/Spark.Web/", "Spark.Web/"]\nCOPY ["./src/Spark.Engine/", "Spark.Engine/"]\nCOPY ["./src/Spark.Mongo/", "Spark.Mongo/"]\nRUN dotnet restore "/src/Spark.Web/Spark.Web.csproj"\nCOPY . .\nRUN dotnet build "/src/Spark.Web/Spark.Web.csproj" -c Release -o /app\n\nFROM build AS publish\nRUN dotnet publish "/src/Spark.Web/Spark.Web.csproj" -c Release -o /app\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app .\n# COPY --from=build /src/Spark.Web/example_data/fhir_examples ./fhir_examples\n\nENTRYPOINT ["dotnet", "Spark.Web.dll"]\n')),Object(r.b)("p",null,"If you want to run the Mongo database as well, you could configure it with a ",Object(r.b)("inlineCode",{parentName:"p"},"docker-compse.yml"),"-file:"),Object(r.b)("pre",null,Object(r.b)("code",a({parentName:"pre"},{}),'version: "3"\nservices:\n  spark:\n    container_name: spark\n    restart: always\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      - StoreSettings__ConnectionString=mongodb://root:CosmicTopSecret@mongodb:27017/spark?authSource=admin\n      - SparkSettings__Endpoint=http://localhost:5555/fhir\n    ports:\n      - "5555:80"\n      - "44348:443"\n    links:\n      - mongodb\n    depends_on:\n      - mongodb\n  mongodb:\n    container_name: mongodb\n    image: mongo\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: root\n      MONGO_INITDB_ROOT_PASSWORD: CosmicTopSecret\n    ports:\n      - "17017:27017"\n  mongosetup:\n    container_name: mongosetup\n    image: mongo\n    volumes:\n      - ./src/Spark.Web/example_data/db_dump:/data/db_dump\n    depends_on:\n      - mongodb\n    links:\n      - mongodb\n    entrypoint:\n      ["mongorestore", "--uri=mongodb://root:CosmicTopSecret@mongodb:27017/spark", "--drop", "--archive=/data/db_dump/dstu2.archive.gz", "--gzip"]\n    environment:\n      WAIT_HOSTS: mongodb:27017\n')))}s&&s===Object(s)&&Object.isExtensible(s)&&Object.defineProperty(s,"__filemeta",{configurable:!0,value:{name:"MDXContent",filename:"src/pages/Get_started/Run_with_Docker.md"}}),s.isMDXComponent=!0}}]);
//# sourceMappingURL=component---src-pages-get-started-run-with-docker-md-b6f61c834e5aa099de6d.js.map